빈 메모리 공간이 많을 때는 문제가 없지만~~
빈 메모리 공간이 거의 없으면 운영체제는 **메모리 압박**을 해소하기 위해 다른 페이지들을 강제적으로 **paging-out** 시켜 활발히 사용 중인 페이지들을 위한 공간을 확보한다. 

어떤 페이지를 내보낼(**evict**)지를 선택하는 것이 운영체제의 교체 정책(**replacement policy**)이다!

## 25. 1 캐시 관리
메인 메모리는 시스템의 가상 메모리 페이지를 가져다 놓기 위한 **캐시**라고 생각할 수 있다. 
이 캐시를 위한 교체 정책의 목표는 **캐시 미스(cache miss)** 를 최소화하는 것, 또는 **캐시 히트(cache hit)** 를 최대화하는 것이라 할 수 있다. 
즉, 디스크로부터 페이지를 가져오는 횟수를 최소로 만들거나, 접근된 페이지가 메모리에 이미 존재하는 횟수를 최대로 하는 것이다. 

캐시 히트와 미스의 횟수를 안다면 프로그램의 **평균 메모리 접근 시간(Average Memory Access Time, AMAT)** 을 계산할 수 있다. 
![IMG-20241226171413.png](IMG-20241226171413.png)
$T_M$ 과 $T_D$ 는 각각 **메모리, 디스크 접근 비용**, $P_{Hit}$ 과 $P_{Miss}$는 각각 **히트율(hit rate), 미스율(miss rate)** 를 나타낸다. 
$AMAT = 메모리 접근 비용 \times 히트율 + 디스크 접근 비용 \times 미스율$ 로 계산되는데, 현대 시스템에서는 디스크 접근 비용이 너무 크기 때문에 $P_{Miss}(미스율)$를 최대한 줄이는 방향으로 정책을 설계해야 한다. 

## 25. 2 여러가지 교체 정책
### 1. 최적 교체 정책
**Page Fault**가 발생했을 때, 미래에 가장 늦게 사용될 페이지를 교체하는 방식
- 모든 페이지 교체 알고리즘 중 **가장 효율적인 성능을 제공**한다. 
- **미래의 페이지 참조 순서를 바탕**으로 **가장 합리적인 결정**을 하므로 이상적인 성능을 나타내는 것이 당연하다. 
- 그러나 실제 시스템에서 미래의 페이지 참조 순서를 미리 아는 것을 **불가능**하다.
따라서 현실적으로 구현할 수 없기에, 주로 **이론적인 성능 비교를 위한 기준**으로 사용된다. 
### 2. FIFO(First In First Out)
매우 간단한 교체 정책이다! **먼저 들어온 것을 먼저 내보내는** 교체 방식이다. 
최적 교체 정책과 비교하면 FIFO는 성능이 많이 떨어진다. 
→ FIFO는 블록들의 중요도를 판단할 수가 없기 때문이다. 
### 3. 무작위 선택
말 그대로 무작위로 내보낼 것을 정한다! 운에 전적으로 의존하기에 성능도 그에 따라 변동이 심하다
### 4. LRU(Least-Recently Used): 과거 정보의 사용
미래에 대한 예측을 위해 과거 사용 이력을 활용한다. 
**Least-Recently Used(LRU)**: 가장 오래전에 사용하였던 페이지를 교체한다. 
**Least-Frequently Used(LFU)**: 가장 적은 빈도로 사용된 페이지를 교체한다. 

## 25. 3 워크로드에 따른 성능 비교
총 100개의 페이지에 대해 10000번 접근 하는 실험을 진행한다. 
캐시의 크기는 매우 작은 것부터(한 페이지) 모든 페이지들을 담을 수 있을 정도의 크기(100페이지)까지 증가 시키며 각 정책이 어떻게 동작하는지 살펴본다. 
### 1. 지역성이 없는 워크로드
이 말은 접근되는 페이지들의 집합에서 페이지가 무작위적으로 참조된 것을 의미한다. 
접근되는 페이지는 무작위적으로 선택되는 것이다. 
![IMG-20241226175503.png](IMG-20241226175503.png)
지역성이 없다면, LRU와 FIFO 그리고 Random Select 모두 동일한 성능을 보이며 어느 정책을 사용하든지 상관이 없다. 
### 2. 80대 20 워크로드
20%의 페이지들(“인기 있는 페이지”)에서 80%의 참조가 발생하고, 나머지 80%의 페이지들(“비인기 페이지”)에 대해서 20%의 참조만 발생한다. 
![IMG-20241226181125.png](IMG-20241226181125.png)
랜덤과 FIFO 정책이 상당히 좋은 성능을 보이지만, 인기 페이지들을 캐시에 더 오래두는 경향이 있는 LRU가 더 좋은 성능을 보인다. 
### 3. 순차 반복 워크로드
50개의 페이지들을 순차적으로 참조한다. 
0~49번째 페이지를 참조한 후에 다시 처음으로 돌아가서 그 접근 순서를 반복한다. 
50개의 개별적인 페이지들을 총 10,000번 접근한다. 
![IMG-20241226181614.png](IMG-20241226181614.png)
캐시 크기가 50이 되기 전까지 LRU와 FIFO정책은 가장 먼저 들어온 page를 내보내게 되기 때문에 적중률이 0% 가 나와버린다! 이런 경우가 있기에 **무작위 선택 정책은 몇 가지 좋은 특성**을 가지고 있다는 것을 알 수 있다. 이상한 코너 케이스가 안나온다는 것이다. 
## 25. 4 LRU 정책 근사하기
**use bit**(a.k.a. **reference bit**)를 사용하면 LRU를 근사하는 식으로 정책을 만들 수 있다. 
페이지가 참조될 때마다 하드웨어에 의해 **use bit**가 1로 설정되게 된다. 

운영체제는 **use bit**를 활용해서 시계 알고리즘(clock algorithm)을 구현할 수 있다.
1. 페이지를 시계 바늘이 이동하면서 **use bit**이 1인지 0인지를 검사한다. 
2. 만약 1이라면 페이지는 최근에 사용되었으며 바람직한 교체 대상이 아니라 판단 후 **use bit**을 다시 0으로 설정한다. 
3. 시계 바늘은 **use bit**가 0으로 설정되어 있는, 즉 최근에 사용된 적이 없는 페이지를 찾을 때까지 반복한다. 
![IMG-20241226182422.png](IMG-20241226182422.png)
물론 완벽한 LRU만큼 좋은 성능을 보이지는 못하지만, 과거 정보를 고려하지 않는 다른 기법들에 비해서는 성능이 더 좋다. 

## 25. 5 다른 VM 정책들
- 어떤 페이지가 곧 사용될지를 대략 예상해서 미리 메모리로 읽어들이는 **prefetching**
  → 성공할 확률이 충분히 높을 때에만 해야 한다. 
- 운영체제가 변경된 페이지를 디스크에 반영 시 **한 번에 한 페이지씩** 디스크에 쓸 수도 있지만, 많은 시스템은 기록해야 할 페이지들을 메모리에 모은 후, 한 번에 디스크에 기록한다. 
  → 이와 같은 동작은 **clustering** 이라고 부른다. 
- 1) <u>메모리 사용 요구가 감당할 수 없을 만큼 많고</u> 2) <u>실행 중인 프로세스가 요구하는 메모리가 가용 물리 메모리 크기를 초과하는 경우</u>에 시스템은 끊임없이 페이징을 할 수 밖에 없고, 이와 같은 상황을 **쓰래싱(thrashing)** 이라고 부른다. 
  → 다수의 프로세스가 존재할 때, 일부 프로세스의 실행을 중지시키는 **진입 제어(admission control)** 을 시행한다. 또는 일부 버전의 Linux에서는 메모리 요구가 초과되면 **메모리 부족 킬러(out-of-memory killer)를 실행**시키기도 한다. 