1. OX 
	1. MC(bootstrapX → 한번에 업데이트) marchov 필요 → x
	2. DP converges O
	3. MC converges O 
2. 
	1. on-learning ← → off learning
	2. one sweep 에 Policy evaluation, policy update 둘 다 있나?
	3. $\epsilon$ 해야하는 이유
3. 