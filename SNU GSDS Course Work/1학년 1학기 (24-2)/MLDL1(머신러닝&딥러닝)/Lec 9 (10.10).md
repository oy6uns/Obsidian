# Ensemble Methods and Boosting
## Bootstrapping vs. Cross-validation
![[IMG-20241010113302.png]]
## Bagging
#### 1. **Bagging (Bootstrap Aggregation) 개념**

- **Bagging**은 "Bootstrap Aggregation"의 줄임말로, **부트스트랩 샘플링과 모델 예측의 결합**을 통해 예측 성능을 향상시키는 방법입니다.
    
- Bagging의 기본 아이디어는 **여러 개의 부트스트랩 샘플**을 생성하고, 각 샘플에 대해 독립적인 모델을 학습시킨 뒤, 이 모델들의 예측을 **결합(평균, 다수결)**하여 최종 예측값을 도출하는 것입니다.
    
- **Bagging의 과정**:
    
    1. **B개의 부트스트랩 샘플 생성**:
        
        - 단일 학습 데이터셋에서 중복을 허용하는 샘플링을 통해 BBB개의 부트스트랩 샘플을 생성합니다.
        - 각 부트스트랩 샘플은 원래 학습 데이터의 약 2/3 정도만을 포함하게 됩니다.
    2. **각 부트스트랩 샘플을 이용한 모델 학습**:
        
        - bbb-번째 부트스트랩 샘플을 사용하여 모델 f∗b(x)f^{*b}(x)f∗b(x)를 학습합니다.
    3. **모든 예측값의 결합**:
        
        - 각 부트스트랩 샘플에서 학습된 모델의 예측값 f∗b(x)f^{*b}(x)f∗b(x)를 평균내어 최종 예측값을 도출합니다.
            
        - 최종 결합된 예측값 f^bag(x)\hat{f}_{\text{bag}}(x)f^​bag​(x)는 다음과 같이 정의됩니다:
            
        
        f^bag(x)=1B∑b=1Bf∗b(x)\hat{f}_{\text{bag}}(x) = \frac{1}{B} \sum_{b=1}^{B} f^{*b}(x)f^​bag​(x)=B1​b=1∑B​f∗b(x)

#### 2. **Bagging의 특징**

- Bagging은 각 모델의 예측값(결과 값)을 결합하는 방식이기 때문에, **중간 변수(Intermediate Variables)를 사용하지 않습니다**.
- 또한, Bagging은 예측값을 단순히 평균내거나 다수결 투표를 통해 결합하기 때문에, **분류(Classification)** 및 **회귀(Regression)** 모델에 모두 적용 가능합니다.
    - **분류 문제**에서는 각 모델의 다수결(Majority Voting) 방식으로 최종 결과를 결정합니다.
    - **회귀 문제**에서는 각 모델의 예측값을 평균내어 최종 결과를 결정합니다.

#### 3. **Bagging의 장점**

- Bagging은 데이터의 분산을 줄이고, 예측 모델의 **과적합(Overfitting)**을 줄이는 데 도움을 줍니다.
- 특히, Decision Tree와 같은 고분산 모델의 성능을 안정화시키고, 다양한 데이터셋에 대해 일관된 예측 성능을 발휘할 수 있도록 돕습니다.

![[IMG-20241010113628.png]]![[IMG-20241010113640.png]]
## Boosting
#### 3. **Boosting의 기본 흐름 (Overall Flow)**

- Boosting은 다음과 같은 단계로 진행됩니다:
    
    1. **약한 학습자 (Weak Learner)들의 결합**:
        
        - Boosting은 여러 개의 약한 학습자들을 결합하여 하나의 강력한 예측 모델을 만듭니다.
        - 이 약한 학습자들은 단일로는 성능이 낮지만, 결합되면 성능이 크게 향상됩니다.
    2. **순차적 결합 (Sequential Combination)**:
        
        - 약한 학습자들은 순차적으로 결합되며, 이전 모델의 성능을 바탕으로 다음 모델이 학습됩니다.
    3. **학습 데이터의 재가중치 (Reweighting Training Examples)**:
        
        - 각 모델의 성능에 따라 **학습 데이터의 가중치**가 조정됩니다.
        - 잘못 분류된 데이터는 다음 모델에서 **가중치가 높아져** 더 중요하게 학습됩니다.
        - 올바르게 분류된 데이터는 가중치가 낮아져 상대적으로 덜 중요하게 학습됩니다.
    4. **반복적인 학습**:
        
        - 위의 과정을 반복하여 여러 개의 약한 학습자를 학습시킵니다.
        - 매 반복마다 전체 모델의 성능을 측정하고, 가중치를 조정합니다.
    5. **최종 결합**:
        
        - 학습된 모든 약한 학습자들은 **선형 결합**으로 최종 예측을 만듭니다.
        - 각 학습자의 가중치는 성능에 비례하여 할당됩니다. 성능이 좋은 학습자는 높은 가중치를, 성능이 낮은 학습자는 낮은 가중치를 가집니다.

#### 4. **Boosting의 장점**

- Boosting은 **이전 모델의 오류를 보완**하는 방식으로 학습을 진행하기 때문에, **편향(Bias)과 분산(Variance) 모두를 줄이는 데 효과적**입니다.
- 성능이 낮은 약한 학습자들을 결합하여 강력한 예측 모델을 만들 수 있습니다.

#### 5. **Boosting의 단점**

- Boosting은 각 모델이 순차적으로 학습되기 때문에, **병렬 처리**가 어렵고 학습 속도가 느릴 수 있습니다.
- 데이터에 **과적합(Overfitting)**되기 쉽습니다. 특히, 모델이 학습 데이터의 노이즈에 과도하게 반응할 수 있습니다.